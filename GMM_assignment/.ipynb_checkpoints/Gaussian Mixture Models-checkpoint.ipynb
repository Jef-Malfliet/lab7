{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models and Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab session, we used distance metrics in a $d$-dimensional feature space to find the best clustering. While k-means is very useful in many situations, in some cases it will still fail. In such case, it can help to take a probabilistic approach and view the items in your dataset as samples from an unknown probabilistic distribution.  pecifically, we will assume that all samples belonging to one cluster are statistically distributed according to a Gaussian (=normal) distribution. Since we have multiple clusters, we need a *mixture* of Gaussians to accurately model the sample distribution.\n",
    "\n",
    "The ML problem is then to learn the parameters of this distribution, e.g. the mean and variance of the Gaussian. The result is a **generative** model that can also be used for anomaly detection, or to generate new data. We will also explore these applications of generative modelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When k-means clustering fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since k-means uses Euclidean distance between features, it was important to apply feature scaling to ensure that your data has zero mean and unit variance along each dimension. But there are many realistic situations where k-means will fail, despite this normalization. This is the case when your clusters have an ellipsoidal shape.\n",
    "\n",
    "<font color='red'>Task: run the code below to load the data and to plot the distribution.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv('Clustering_gmm.csv')\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(data[\"Weight\"],data[\"Height\"])\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Height')\n",
    "plt.title('Data Distribution')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task: \n",
    "* normalize your features with a <tt>StandardScaler</tt> and apply k-means with 4 clusters. \n",
    "* Visualize the clustering with the method provided below. Make separate plots for the scaled and for the unscaled data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "#use this function to visualize the clustering\n",
    "def plotClusters2D(data, cluster_labels, cluster_centers = None):  \n",
    "    '''\n",
    "    This function plots the data items, using a different color per cluster. If cluster_centers are provided, these are also plotted.\n",
    "    \n",
    "    Args:\n",
    "    * data: Numpy array of tuples: [[item1_x, item1_y], [item2_x, item2_y] ,... ]\n",
    "    * cluster_labels: an array of integers containing the cluster number for each data item: \n",
    "    * cluster_centers: optional, an array of tuples containing the feature values of the cluster centers [[center1_x, center1_y], [center2_x,center 2_y]...]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    plt.figure() \n",
    "    unique_labels = set(cluster_labels)\n",
    "    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "          # Black used for points not assigned to a cluster (will be useful for other algorithms, k-means assigns each element to a cluster)\n",
    "          col = [0, 0, 0, 1]        \n",
    "        \n",
    "        plt.scatter(data[cluster_labels == k,0],data[cluster_labels == k,1],s=20,c=[col])\n",
    "\n",
    "    if cluster_centers is not None:        \n",
    "        plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], s=250, marker='*', c='red', edgecolor='black')\n",
    "  \n",
    "    plt.xlabel('Feature 1') ;   \n",
    "    plt.ylabel('Feature 2');\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after normalization, the result of k-means is clearly unsatisfactory. K-means faces two problems:\n",
    "* data points are assigned to the closest cluster center. This means that **k-means cannot produce ellipsoidal cluster shapes**. You observe this effect in the two clusters in the middle. The separation line indicates the points at equal distance from both centroids.\n",
    "* visually, we cluster based on the density of the data points. White areas in feature space, where there are no data points, are an indication of cluster boundaries. But **k-means does not account for differences in density**.\n",
    "* k-means provides hard clustering: each data point is assigned to precisely one cluster. But imagine you have a person that weighs 63kg and is 171 cm tall. This person could belong to any of the two groups, so your clustering is quite uncertain. **k-means does not give an indication of uncertainty**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing your data through a probabilistic lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all features in your dataset are continuous, you can think of each sample as a point in an $n$-dimensional space. In most realistic datasets, these points will not be uniformly over the feature space: some regions will contain more points than others. \n",
    "\n",
    "When clustering, we will make the assumption that the samples of each cluster are Gaussian distributed. The underlying idea is that there is always some variation between all individuals of the same group. Our goal will then be to identify the **number** of Gaussians (groups) in the dataset, and to estimate the mean and variance of the Gaussian distribution of each group.\n",
    "\n",
    "Let us start by visualizing the idea that features of members of an individual group are Gaussian distributed. We will use the Iris dataset containing 150 feature vectors of iris flowers. There are 50 samples for each of three types of iris flowers: *setosa*, *versicolor* and *virginica*. We will use these labels to validate the result of the clustering.\n",
    "\n",
    "The features relate to the width and length of the two leaf types of an iris, as shown below.\n",
    "\n",
    "![petal](./images/petal_sepal.jpg)\n",
    "\n",
    "<font color='red'>Task: load the data set with the code below</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "iris = load_iris()\n",
    "iris_data = pd.DataFrame(np.concatenate((iris.data, np.array([iris.target]).T), axis=1), columns=iris.feature_names + ['target'])\n",
    "print(\"Types of iris flowers: \",iris.target_names)\n",
    "iris_data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Task: Inspect the feature distribution of each flower type separately by making *separate* pairplots per type. Put the feature histogram on the diagonal of each pairplot. Pairplot is in the <tt>Seaborn</tt> module. If needed, set the scale of the Y-axis to the interval [0,6].\n",
    "\n",
    "* Can you discern (approximately) a normal distribution in the histogram of most features?\n",
    "* Can you observe ellipsoidal regions in the scatterplots with high density of data points? Since all features are in cm, the notion of distance is the same along all axes and the ellipses will not disappear by rescaling.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#you can use sns.pairplot here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's continue working on only the data of the \"setosa\" type (target==0). Imagine to divide each axis in the 4-D feature space in equal intervals. This creates in imaginary grid of 4-D cubes. You can now count the number of samples in your dataset that fall in each bin. If your dataset is sufficiently large, the fraction of samples in each bin becomes an indication of the probability that an unseen iris flower will have a combination of feature values that falls in this bin.\n",
    "\n",
    "Visualization is only possible in 2D. Let's create an imaginary grid over the 2-dimensional space spanned by the features of sepal length and width, and count the number of samples in each (square) bin. You can then create a 3D bar plot that shows what fraction of the samples in the dataset falls in each bin.\n",
    "\n",
    "<font color='red'>Task: Run the code below to visualize the binning result.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "iris_data.loc[iris_data['target']==0.0,'sepal length (cm)']\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "hist, xedges, yedges = np.histogram2d(iris_data.loc[iris_data['target']==0.0,'sepal length (cm)'].to_numpy(), iris_data.loc[iris_data['target']==0.0,'sepal width (cm)'].to_numpy(), bins=150, range=[[3, 8], [1, 6]])\n",
    "\n",
    "# Construct arrays for the anchor positions of the 16 bars.\n",
    "xpos, ypos = np.meshgrid(xedges[:-1] + 0.25, yedges[:-1] + 0.25, indexing=\"ij\")\n",
    "xpos = xpos.ravel()\n",
    "ypos = ypos.ravel()\n",
    "zpos = 0\n",
    "\n",
    "# Construct arrays with the dimensions for the 16 bars.\n",
    "dx = dy = 0.5 * np.ones_like(zpos)\n",
    "N=iris_data[iris_data['target']==0.0].shape[0]\n",
    "dz = 1/N*hist.ravel() #normalize\n",
    "\n",
    "ax.bar3d(xpos, ypos, zpos, dx, dy, dz, zsort='max')\n",
    "ax.set_xlabel('sepal length (cm)')\n",
    "ax.set_ylabel('sepal width (cm)')\n",
    "ax.set_zlabel('fraction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By dividing the counts of each bin with the total number of elements in the dataset, you get an estimate of the probability that a random sample in your dataset falls into a particular bin. This is obviously only an approximation of the true probability, but the more data we have, the better these estimates will be.\n",
    "\n",
    "Our current bins are discrete. But if we would have more data, we could reduce the bin size. If we would have a very large dataset, we could make the bin size very small. The tops of all bars would then form a surface. In the limit, we would get a continuous function, the **probability density function** $p_{data}(x_1,x_2,x_3,x_4)$, with $x_i$ continuous variables referring to the sepal length, width, petal length and petal width respectively. Usually our data is of much higher dimension $d$, and we use a shorthand notation:\n",
    "$$\n",
    "p_{data}(\\textbf{x}) \\textrm{ with } \\textbf{x} = [x_1,x_2,...,x_d]\n",
    "$$\n",
    "\n",
    "We again remind you that the **probability density function** can take values larger than 1. The only requirement is that its integral over all possible feature values sums to 1. \n",
    "\n",
    "Our goal will be to fit from our dataset a continuous function $p_{model}$ that is a good approximation of the unknown density distribution $p_{data}$. In our example, we will seek for a probability density distribution $p_{model}(\\textrm{sepal length},  \\textrm{ sepal width},  \\textrm{ petal length},  \\textrm{ petal width})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric families of probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, there are an infinite number of possible probability density functions $p_{model}$. Any continuous function is a valid candidate, on condition that the integral of this function over all possible values of $x$ equals to 1. This leaves us with a lot of candidates.\n",
    "\n",
    "To help our Machine Learning algorithm, we need to reduce the set of candidate functions. We do this by selecting a **family of probability distributions**. Each member of this family is a function with the same mathematical function, but a different value for its parameter set. This parameter set is denoted by $\\mathbf{\\theta} = [\\theta_1, \\theta_2, ... \\theta_p]$.\n",
    "The ML algorithm then only needs to estimate the correct value of the vector $\\mathbf{\\theta}$. To explicitly indicate that the algorithm is trying to find a function within a family of functions, the following notation is used:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{model}(\\mathbf{x};\\mathbf{\\theta}) &= p_{model}(x_1, x_2, ..., x_d;\\mathbf{\\theta}) \\\\\n",
    "&= p_{model}(x_1, x_2, ..., x_d;\\theta_1,\\theta_2,...,\\theta_p) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Mind the semicolon. All letters before the semicolon refer to the features in our dataset. All letters after the semicolon refer to parameters of the chosen family. \n",
    "\n",
    "For instance, if we have onedimensional data, we could choose for the family of exponential distributions, with $\\mathbf{\\theta} = [\\lambda]$. All negative values of $x$ have a probability of zero. For positive values of $x$, the following function is used:\n",
    "<br/><br/>\n",
    "\n",
    "$$\n",
    "p_{model}(x;\\lambda) = \\lambda {\\rm e}^{-\\lambda x}\n",
    "$$\n",
    "    \n",
    "Another (very common) choice would be the family of Gaussian (or normal) distributions, with $\\mathbf{\\theta} = [\\mu, \\sigma]$.\n",
    "\n",
    "$$\n",
    "p_{model}(x;\\mu,\\sigma) = \\mathcal{N}(\\mathbf{x};\\mu,\\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}{\\rm e}^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n",
    "$$\n",
    "\n",
    "<font color='red'>Task: plot a few members of each family. Experiment with different values for the parameter to understand their effect on the shape of the distribution.</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon,norm\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "axes[0].set_title(\"Family of exponential distributions\")\n",
    "axes[1].set_title(\"Family of Gaussian distributions\")\n",
    "\n",
    "#TO DO: choose the values to plot\n",
    "values=[...,...]\n",
    "for par_lambda in values:        \n",
    "    xv = np.linspace(0.0001,4, 100);\n",
    "    sns.lineplot(x=xv,y=expon.pdf(xv,scale=1/par_lambda),ax=axes[0]);    \n",
    "axes[0].legend(values,title='$\\lambda$');\n",
    "    \n",
    "#TO DO: choose the values to plot\n",
    "mu=[...,...]\n",
    "sigma=[...,...]\n",
    "legend_titles = []\n",
    "for par_mu in mu:\n",
    "    for par_sigma in sigma:\n",
    "        xv=np.linspace(0,10,100);\n",
    "        sns.lineplot(x=xv, y=norm.pdf(xv,loc=par_mu,scale=par_sigma),ax=axes[1])\n",
    "        legend_titles.append(f\"$\\mu = {par_mu}$, $\\sigma^2 = {par_sigma}$\")\n",
    "axes[1].legend(legend_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of the multivariate Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presented examples were all families of univariate distributions: they only model the statistical distribution of a single feature. In ML, we work with feature vectors of higher dimensionality. In $d$-dimensions, the probability density function of the multivariate Gaussian is given by:\n",
    "\n",
    "$$\n",
    "p_{model}(\\mathbf{x};\\mathbf{\\mu},\\mathbf{\\Sigma}) = \\frac{1}{\\sqrt{\\left(2 \\pi \\right)^{d} |\\Sigma|}}\\rm{exp}\\left(-\\frac{1}{2}\\left(\\mathbf{x} - \\mathbf{\\mu}\\right)^T \\mathbf{\\Sigma^{-1}}\\left(\\mathbf{x}-\\mathbf{\\mu}\\right)\\right)\n",
    "$$\n",
    "\n",
    "This is a probability **density** function, which means it can take values larger than 1 for some values. This also means that you cannot interpret the value at a particular point to be the probability of that point.\n",
    "\n",
    "In this notation, $\\mathbf{x}$ represent a column vector of dimension $d$ (usually, we work with row vectors!)\n",
    "$\\mathbf{x} = \\begin{bmatrix} \n",
    "x_1 \\\\ \n",
    "x_2 \\\\ \n",
    "... \\\\\n",
    "x_d\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "The parameters of this distribution are the column vector of dimension $d$ with averages along each axis: $\\mathbf{\\mu} = \\begin{bmatrix} \n",
    "\\mu_1 \\\\ \n",
    "\\mu_2 \\\\ \n",
    "... \\\\\n",
    "\\mu_d \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and a $d \\times d$ **covariance matrix** containing the covariance between each pair of dimensions:\n",
    "$\n",
    "\\mathbf{\\Sigma} = \\begin{bmatrix} \n",
    "\\sigma_{11} & \\sigma_{12} & ... \\sigma_{1d} \\\\ \n",
    "\\sigma_{21} & \\sigma_{22} & ... \\sigma_{2d} \\\\ \n",
    "... \\\\\n",
    "\\sigma_{d1} & \\sigma_{d2} & ... \\sigma_{dd} \\\\ \n",
    "\\end{bmatrix}$ with determinant det($\\Sigma$)=|$\\Sigma$|, \n",
    "\n",
    "The covariancematrix should be invertible. Therefore, we will always use *symmetric* matrices:\n",
    "$\n",
    "\\mathbf{\\Sigma} = \\begin{bmatrix} \n",
    "\\sigma_{11} & \\sigma_{12} & ... \\sigma_{1d} \\\\ \n",
    "\\sigma_{12} & \\sigma_{22} & ... \\sigma_{2d} \\\\ \n",
    "... \\\\\n",
    "\\sigma_{1d} & \\sigma_{2d} & ... \\sigma_{dd} \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "<font color='red'>Task: \n",
    "* use the plot code below to better understand the effect of the parameter values for the mean and covariance of a 2-D Gaussian. The plot shows in 3D the value of the density function. This is also projected on the 2D surface in a so called **density plot**.\n",
    "* The mean vector determines the center of the distribution, whereas the covariance matrix determines the shape of the ellipse. Adjust the parameters of the covariance matrix to find a density function with\n",
    "  * circular shape\n",
    "  * ellipsoidal shape, with the longest axis aligned with the X-axis\n",
    "  * ellipsoidal shape, with the longest axis having a slope of 45 degrees with the X-axis.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Our 2-dimensional distribution will be over variables X and Y\n",
    "N = 60\n",
    "X = np.linspace(-4, 4, N)\n",
    "Y = np.linspace(-4, 4, N)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "#TO DO: choose a mean vector. (How many elements do you need for a 2D Gaussian?)\n",
    "mu = np.array([...])\n",
    "#TO DO: choose a covariance matrix. Make sure it is symmetric!\n",
    "Sigma = np.array([[...], [...]])\n",
    "\n",
    "# Pack X and Y into a single 3-dimensional array\n",
    "pos = np.empty(X.shape + (2,))\n",
    "pos[:, :, 0] = X\n",
    "pos[:, :, 1] = Y\n",
    "\n",
    "def multivariate_gaussian(pos, mu, Sigma):\n",
    "    \"\"\"Return the multivariate Gaussian distribution on array pos.\n",
    "\n",
    "    pos is an array constructed by packing the meshed arrays of variables\n",
    "    x_1, x_2, x_3, ..., x_k into its _last_ dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n = mu.shape[0]\n",
    "    Sigma_det = np.linalg.det(Sigma)\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    N = np.sqrt((2*np.pi)**n * Sigma_det)\n",
    "    # This einsum call calculates (x-mu)T.Sigma-1.(x-mu) in a vectorized\n",
    "    # way across all the input variables.\n",
    "    fac = np.einsum('...k,kl,...l->...', pos-mu, Sigma_inv, pos-mu)\n",
    "\n",
    "    return np.exp(-fac / 2) / N\n",
    "\n",
    "# The distribution on the variables X, Y packed into pos.\n",
    "Z = multivariate_gaussian(pos, mu, Sigma)\n",
    "\n",
    "# Create a surface plot and projected filled contour plot under it.\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True,\n",
    "                cmap=cm.viridis)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "cset = ax.contourf(X, Y, Z, zdir='z', offset=-0.15, cmap=cm.viridis)\n",
    "\n",
    "# Adjust the limits, ticks and view angle\n",
    "ax.set_zlim(-0.15,0.2)\n",
    "ax.set_zticks(np.linspace(0,0.2,5))\n",
    "ax.view_init(27, +45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our roadmap "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our assumption is that our data contains several clusters, and that the samples of each cluster are distributed with a member of the Gaussian family. We thus need to find both the number of clusters, as well as the parameter $\\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$ for each of the cluster distributions.\n",
    "\n",
    "We will tackle the problem in steps of increasing complexity:\n",
    "* one-dimensional data, one cluster: First, we will assume that our samples have only a single feature, and that all samples in our dataset belong to the same cluster. You will learn how to use Maximum Likelihood Estimation to find the parameters of this distribution.\n",
    "* one-dimensional data, multiple cluster: Then, we will learn how to estimate both the number of clusters as the parameters of each cluster distribution\n",
    "* multi-dimensional data, multiple clusters: this is the most generic setting.\n",
    "\n",
    "For the first two steps, we will look into a realistic application: Non-Intrusive Load Monitoring. In the last step, we will come back to our Iris flower dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One dimension, one Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Non-Intrusive Load Monitoring (NILM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NILM aims to break down the overall power consumption of a household into power profiles of individual appliances. The technique is called non-intrusive since it does not require to install sensors on each device. The principle is illustrated in the figure below. Identifying individual appliances has applications in e.g. electrical grid balancing where one can activate additional appliances if the power consumption of solar panels is high. \n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/nilm_principle.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<figcaption> Image taken from Aladesanmi and Folly, Overview of non-intrusive load monitoring and identification techniques. </figcaption>\n",
    "</figure>\n",
    "\n",
    "Switching a device on or off will result in a jump or drop of the total power consumption. The height of this difference is specific to each device. Therefore, we will first model the distribution of these differences for individual appliances with a Gaussian distribution. One has collected training data by sensors mounted on individual appliances.\n",
    "\n",
    "Once this distribution is learned for each applicance, we could limit ourselves to monitoring the overall power switch (hoofdschakelaar). Each time a jump or drop in the power consumption is observed, we could then use these distributions to calculate the probability that this new event is due to a particular device. The switching event could then e.g. be classified as belong to the class for which the highest probability density was found. \n",
    "\n",
    "We will make use of the REDD dataset, which contains the overall power consumption of several households, as well as individual profiles for several appliance types. This dataset can be easily explored and manipulated with the convenient nilmtk package.\n",
    "Let's explore the dataset first. \n",
    "\n",
    "<font color='red'>Try to answer the following questions from the metadata. \n",
    "* In how many households was data collected? \n",
    "* How was the aggregate (total) load measured? \n",
    "* How was the individual load measured?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one time conversion of raw data to h5 format - you should have done this in preparation of the lab\n",
    "#from nilmtk.dataset_converters import convert_redd\n",
    "#convert_redd('./low_freq', './redd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilmtk import DataSet\n",
    "from nilmtk.utils import print_dict\n",
    "\n",
    "\n",
    "redd=DataSet('./redd.h5')\n",
    "print_dict(redd.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on the data of a single household (instance 1). \n",
    "\n",
    "<font color='red'>Try to answer the following questions:\n",
    "* How many devices were used to measure the site energy consumption?\n",
    "* What appliances were in the house?\n",
    "* Which appliances were measured with multiple meters?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict(redd.buildings)\n",
    "elec=redd.buildings[1].elec\n",
    "print(elec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of power differences due to oven switch events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Plot the power consumption of the electric oven. </font>\n",
    "\n",
    "Hint: you can select an appliance easily by indexing in the <tt>MeterGroup</tt> object based on instance number. This will select an <tt>ElecMeter</tt> object, which has a <tt>plot</tt> method. See the plot method of the <tt>Electric</tt> class, which is the base class of <tt>ElecMeter</tt>: [documentation](http://nilmtk.github.io/nilmtk/master/nilmtk.html?#nilmtk.electric.Electric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from nilmtk import MeterGroup\n",
    "\n",
    "#instances to plot\n",
    "plt.figure(figsize=(20,20));\n",
    "plt.title('Electric oven');\n",
    "##TO DO: fill in the correct instance number\n",
    "elec[...].plot();  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task:\n",
    "* Load the raw data of the oven and inspect this data. \n",
    "* Create a dataframe containing the **absolute value** of the difference in active power between subsequent rows. \n",
    "* Plot a histogram of these differences, using 100 bins.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO DO: fill in the correct instancenumber\n",
    "#the active power is found in oven_data.power.active\n",
    "#dataframes have a built-in method to show a histogram. The number of bins can be provided as argument.\n",
    "\n",
    "oven_data=next(elec[...].load());\n",
    "#calculate differences here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is clearly dominated by a large number of very small differences. These differences are simply reflecting the noise and not indicative for switching events. Remember that we are studying the data of an oven, which typically consumes several hundreds of Watts.\n",
    "We should thus discard all power differences below a particular threshold. Instead of intuitively selecting this threshold value, we will follow a more principled approach.<br><br>\n",
    "\n",
    "<font color='red'>\n",
    "Plot an empirical cumulative distribution function, which shows the number of items in your dataset that is larger than a particular value. Use the function <a href=\"https://seaborn.pydata.org/generated/seaborn.ecdfplot.html\n",
    "\"> ecdfplot</a> of the Seaborn library.\n",
    "\n",
    "Use the count statistic and make sure to plot the complementary CDF. Determine an optimal threshold value by looking for an elbow. To properly inspect the graph, you have to zoom in on the lower part of the Y-axis.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20,10));\n",
    "#use sns.ecdfplot\n",
    "#set y limit below\n",
    "plt.ylim(...,...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Choose a threshold, and plot a new histogram on your filtered data. Use 100 bins. Do you agree that the (univariate) Gaussian distribution seems to be a good choice?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method: Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you are convinced that the family of Gaussian distributions is a good choice for this problem. All that is left is to find the value for the parameters $\\mathbf{\\theta} = [\\mu, \\sigma]$ that best explains our given dataset.\n",
    "\n",
    "The best probability function we can find is that function in our family that maximizes the **joint** probability of all items in our dataset. But usually, we assume that all our datasets are identically and independently distributed, an assumption that usually holds in practice. This means that we can describe the joint probability as the product of the individual assumptions. This product is called the **likelihood** function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\mathbf{\\theta};\\mathbf{x_{data}}) = \\prod_{i=1}^n p_{model}(x^{(i)};\\mathbf{\\theta})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the above formula $x^{(i)}$ denotes the $i$-th sample in our dataset consisting of $n$ samples.\n",
    "Observe that this likelihood is only a function of $p$ parameters $\\mathbf{\\theta} = [\\theta_1,...,\\theta_p]$.\n",
    "\n",
    "To find the values $\\hat\\theta$ that maximize the likelihood, you can use the Lagrangian method and solve this set of equations:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{\\theta};\\mathbf{x_{data}})}{\\partial \\theta_1}  &= 0 \\\\\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{\\theta};\\mathbf{x_{data}})}{\\partial \\theta_2}  &= 0 \\\\\n",
    "...\\\\\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{\\theta};\\mathbf{x_{data}})}{\\partial \\theta_p}  &= 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "The likelihood is a product of small numbers, which may lead to numerical instabilities. Taking a logarithm of a product is the sum of the logarithms. Because $\\log(x)$ is a strictly increasing function, maximizing the logarithm of the likelihood will give the same result, but now a summation must be maximized. Another advantage of the logarithm is that it will cancel out the exponentials that are present in probability density functions like the exponential and Gaussian distribution. Numerically, minimization of a function is also more tractable than maximization of a function. For all these reasons, ML experts will therefore find the optimal parameters by minimizing the **negative loglikelihood**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " \\hat{\\theta} &=\\underset{\\mathbf{\\theta}}{\\operatorname{argmax}} \\mathcal{L}(\\mathbf{\\theta};\\mathbf{x_{data}}) \\\\\n",
    "              &=\\underset{\\mathbf{\\theta}}{\\operatorname{argmin}} -\\mathcal{L}(\\mathbf{\\theta};\\mathbf{x_{data}}) \\\\\n",
    "              &=\\underset{\\mathbf{\\theta}}{\\operatorname{argmin}} -\\log\\left(\\mathcal{L}(\\mathbf{\\theta};\\mathbf{x_{data}})\\right)\\\\\n",
    "              &= \\underset{\\mathbf{\\theta}}{\\operatorname{argmin}} -\\sum_{i=1}^n \\log\\left(p_{model}(x^{(i)};\\mathbf{\\theta}\\right)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When choosing for the Gaussian distribution family, and assuming there are $N$ samples in our (filtered) dataset, the loglikelihood is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{LL}(\\mu,\\sigma) &= \\sum_{i=1}^N \\log\\left(p_{model}(x^{(i)};\\mathbf{\\theta}\\right) \\\\\n",
    "                         &= \\sum_{i=1}^N \\log\\left(\\frac{1}{\\sqrt{2\\pi} \\sigma} {\\rm e}^{-\\frac{(x^{(i)}-\\mu)^2}{2 \\sigma^2}}\\right) \\\\\n",
    "                         &= -\\frac{N}{2}\\log(2\\pi) - N\\log(\\sigma) - \\sum_{i=1}^N \\frac{(x^{(i)}-\\mu)^2}{2 \\sigma^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<font color='red'>\n",
    "Define a function that takes as argument values for $\\mu$ and $\\sigma$, and that calculates the **negative** log-likelihood of our dataset (after thresholding). Run this function for a number of randomly chosen arguments. \n",
    "    \n",
    "* Is it correct to state that the $\\mathcal{LL}$ is a probability function? Why (not)?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do\n",
    "def nll(mu, sigma):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Run the code below to create a contourplot and a surfaceplot of the negative log-likelihood. The code assumes that your data is in a dataframe <tt>diff_oven</tt>. For visualization purposes, we plot the log of the value of the loglikelihood. \n",
    "\n",
    "* Make a visual estimation of the values of $\\mu$ and $\\sigma$ that minimize the negative log-likelihood\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "  \n",
    "#code for creating a contourplot\n",
    "#values for meshgrid and levels were tweaked for educational purposes\n",
    "vnll=np.vectorize(nll)\n",
    "xv, yv= np.meshgrid(np.linspace(1000, 2000, 200),np.linspace(100, 500, 200))\n",
    "zv=np.log(vnll(xv,yv)) #because of the huge difference in LL values, we will plot the log of the value\n",
    "contour = plt.contourf(xv,yv,zv,levels=200,cmap=cm.coolwarm);        \n",
    "contour.ax.set(xlabel=\"$\\mu$\",ylabel=\"$\\sigma$\")\n",
    "contour.ax.xaxis.label.set_fontsize(14)\n",
    "contour.ax.yaxis.label.set_fontsize(14)\n",
    "plt.title('$\\log(-\\mathcal{LL})$')\n",
    "\n",
    "#code for creating 3D surface plot\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': '3d'})\n",
    "ax.plot_surface(xv, yv, zv, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "ax.set_xlabel('$\\mu$',fontsize=14)\n",
    "ax.set_ylabel('$\\sigma$',fontsize=14)\n",
    "ax.set_zlabel('$\\log(-\\mathcal{LL})$',fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Visual inspection is obviously quite inaccurate.\n",
    "* Use the code below to numerically determine the minimum of the negative loglikelihood.\n",
    "* Calculate the mean and standard deviation of your dataframe with built-in methods. For the standard deviation, you should set <tt>ddof=0</tt>. These results should compare with numerically calculated minimum. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import scipy.stats\n",
    "#the minimize function requires a signature with a single argument\n",
    "def nll_par(par):    \n",
    "    m = par[0]   \n",
    "    s = par[1]\n",
    "    #to do: add here return <your method(m,s)>    \n",
    "\n",
    "initParams = [1, 1]\n",
    "results = minimize(nll_par, x0=np.array(initParams), method='Nelder-Mead')\n",
    "[mu_est, sigma_est] = results.x\n",
    "print('Estimated parameter values [\\u03BC,\\u03C3] = ',results.x)\n",
    "\n",
    "\n",
    "#to do: calculate directly mean and standard deviation using Dataframe methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can indeed prove that solving the set of partial derivatives of the $\\mathcal{LL}$ of the Gaussian family results in the familiar estimation of mean and sigma, see [this blogpost](http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html). There is no need to use the loglikelihood method when working with a univariate Gaussian distribution. However, the MLE method is generic and is the only one we can use when using more complex distributions, such as Gaussian mixtures and multivariate distributions. \n",
    "\n",
    "Let's now have a look to a situation when there are two clusters in the data. For this, we will estimate the parameters of a mixture of Gaussians.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One dimension, multiple Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of power differences measured on a light circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most households, the light circuits are separated from socket circuits. Let's first explore this data by plotting the power distribution over time of the meter that corresponds with the first light circuit (instance 9). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "from datetime import datetime\n",
    "\n",
    "rcParams['figure.figsize']=(13,10);\n",
    "elec[9].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot should it make clear for you that there are multiple light devices connected to this circuit, each with their own distribution of power differences.\n",
    "\n",
    "<font color='red'>Task:\n",
    "* Create a dataframe that contains the absolute values of the active power difference between subsequent samples.\n",
    "* Plot a histogram of the power differences. The histogram is again dominated by a large number of noise. Use the approach with the ECDF function to determine an appropriate threshold value and filter your data. Choose an appropriate range for the Y-axis, and cut-off at the first elbow.\n",
    "* In the plot above, you also observe very high peaks. These peaks are due to electrical effects that occur when opening a closed circuit, or vice versa. Filter out these high values as well (inspect the elbows on your ECDF).\n",
    "* Plot a histogram of your filtered data. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_data=next(elec[9].load());\n",
    "#to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram has multiple peaks. We call such local maxima the **modes** of the distribution. The Gaussian distribution  has only a single mode, so it is unlikely that a density estimation with this family will provide good results. We need a **multimodal** distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture\n",
    "The data collected on this meter is actually the result of multiple lights. Each light (likely) corresponds to one mode in the distribution of the power differences on this circuit. Some lights are more frequently switched on or off than others (think about your own house). The heights of the modes are determined by how often a particular light is switched on or off. \n",
    "\n",
    "A commonly used multimodal distribution is a mixture of Gaussians. A Gaussian mixture is a weighted linear combination of unimodal Gaussian distributions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) = \\sum_{i=1}^k \\phi_i \\mathcal{N}(x;\\mu_i,\\sigma_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To have a valid probability distribution, the mixture proportions $\\phi_i$ should sum to 1: $\\sum_{i=1}^k \\phi_i = 1$.\n",
    "<br><br>\n",
    "<font color='red'>Plot some examples of the Gaussian distribution below. What effects result from changing $\\mu$, $\\sigma$ and $\\phi$?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon,norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 10))\n",
    "\n",
    "# Put values for mu, sigma and phi for a mixture of 3 gaussians\n",
    "values=[[(1,5,8),(1,1,1),(0.2,0.4,0.2)],\n",
    "\n",
    "        # TODO add values for mu (left), sigma(middle) and phi (right)\n",
    "        [(...),(...),(...)],\n",
    "        [(...),(...),(...)]\n",
    "       ]\n",
    "\n",
    "xv=np.linspace(-5,15,num=200)\n",
    "for index,(m,s,w) in enumerate(values):\n",
    "    sns.lineplot(x=xv,y=w[0]*norm.pdf(xv,loc=m[0],scale=s[0])+w[1]*norm.pdf(xv,loc=m[1],scale=s[2])+w[2]*norm.pdf(xv,loc=m[2],scale=s[2]),ax=axes[index])\n",
    "    axes[index].set_ylim(-0.01,0.2)\n",
    "    axes[index].legend(['\\u03BC = ('+str(m[0])+', '+str(m[1])+', '+str(m[2])+')'+'\\u000a\\u03a6 = ('+str(w[0])+', '+str(w[1])+', '+str(w[2])+')' \n",
    "    +'\\u000a\\u03C3 = ('+str(s[0])+', '+str(s[1])+', '+str(s[2])+')'  ])   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method: Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying our loglikelihood estimation on the Gaussian Mixture distribution family, we need to estimate the means, variances and mixture proportions of the Gaussians.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{model}(x;\\mathbf{\\theta}) &= p_{model}(x;[\\phi_1, \\mu_1, \\sigma_1, ... , \\phi_k, \\mu_k, \\sigma_k]) \\\\\n",
    "                     &= \\sum_{j=1}^k \\phi_j \\mathcal{N}(\\mu_j,\\sigma_j) \\\\\n",
    "\\mathcal{LL}(\\phi_1, \\mu_1, \\sigma_1, ... , \\phi_k, \\mu_k, \\sigma_k) &= \\sum_{i=1}^N \\log\\left(\\sum_{j=1}^k \\phi_j \\mathcal{N}(x^{(i)};\\mu_j,\\sigma_j) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This function is difficult to optimize, because the sum of components appears *inside* the log. The parameters are coupled: there are no separate terms containing only $\\phi_i$ and $\\mu_i, \\sigma_i$.\n",
    "\n",
    "Fortunately, there exists an iterative algorithm to estimate the parameters: the **expectation-maximization step**. It assumes that the data is generated from $k$ hidden (also called **latent**) sources. You can think of this like tossing up a $k$-side coin, and the outcome determines from which source the data came. The data of each source is normally distributed with parameters $\\mu_i$ and $\\sigma_i$. In our example, you could think of the different lights on the circuit\n",
    "with (initially equal) probabilities $\\pi_i$ that a datasample $x$ is generated by switching light $i$. \n",
    "\n",
    "Intuitively, the problem comes down to assigning samples to one of the $k$ Gaussians with a particular probability, and then finding the optimal mean and standard deviation that maximizes the likelihood of that cluster. \n",
    "\n",
    "Finding these parameters is done with the **expectation-maximization** algorithm. The algorithm iterates between two steps, the expectation step and the maximization step. It alternates between finding the probabilities for each point to be generated by each mixture and fitting the mixture to these assigned points. \n",
    "\n",
    "<font color='red'>Task:\n",
    "* Fit a Gaussian Mixture model on the data of the light circuit with the <tt>GaussianMixture</tt> class of the <tt>sklearn.mixture</tt> package. This method executes the EM algorithm and expects the number of components as argument. Inspect your histogram and choose a sensible value. \n",
    "* The function below allows you to plot the density function of your Gaussian Mixture. Compare it with your histogram. does it correspond?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm_light= GaussianMixture(#TO DO: complete here)\n",
    "\n",
    "def plot_estimatedGMM(gmm):\n",
    "    #cov_reshaped=np.reshape(covariances_,(-1,1))\n",
    "    [samples,clusters] = gmm.sample(1000)\n",
    "    min_samples=samples.min()\n",
    "    max_samples=samples.max()    \n",
    "    sns.lineplot(x=samples.reshape(-1),y=(np.exp(gmm.score_samples(samples))).reshape(-1))    \n",
    "\n",
    "plot_estimatedGMM(gmm_light)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you observe two new jumps in the voltage, one of 45V and one of 60 V. \n",
    "\n",
    "<color font='red'>To what cluster number (~light) should you attribute this new sample? Use the <tt>predict</tt> and <tt>predict_proba</tt> methods of your trained <tt>GaussianMixture</tt>. Can you interpret the output of both methods?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsample=np.array([30, 60]).reshape(-1,1);\n",
    "#to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple dimensions, multiple Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to tackle the most generic problem in $d$-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extension of a Gaussian Mixture with $k$ components to multi-dimensional feature vectors is straightforward. Here again, the logarithm and exponential in the loglikelihood will nicely cancel eachother out.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{model}(\\mathbf{x};\\mathbf{\\theta}) &= \\sum_{j=1}^k \\phi_j \\mathcal{N}(\\mathbf{x};\\mathbf{\\mu_j},\\mathbf{\\Sigma_j)} \\\\\n",
    "                                      &= \\sum_{j=1}^k \\phi_j \\frac{1}{\\sqrt{\\left(2 \\pi \\right)^{d} |\\Sigma|}}\\rm{exp}\\left(-\\frac{1}{2}\\left(\\mathbf{x} - \\mathbf{\\mu_j}\\right)^T \\mathbf{\\Sigma_j^{-1}}\\left(\\mathbf{x}-\\mathbf{\\mu_j}\\right)\\right) \\\\\n",
    "\\mathcal{LL}\\left(\\phi_1, ... \\phi_k, \\mathbf{\\mu_1}, ... , \\mathbf{\\mu_k}, \\mathbf{\\Sigma_1}, ... ,\\mathbf{\\Sigma_k}\\right) &= \\sum_{i=1}^N \\log\\left(\\sum_{j=1}^k \\phi_j \\mathcal{N}(\\mathbf{x^{(i)}};\\mathbf{\\mu_j},\\mathbf{\\Sigma_j)}  \\right)\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the large number of parameters that have to be estimated by the MLE method. Each vector $\\mathbf{\\mu_j}$ contains $d$ parameters, and each symmetric matrix $\\mathbf{\\Sigma}_j$ contains $\\frac{d(d+1)}{2}$ parameters. When there are many dimensions, many clusters or few instances, the Expectation Maximization Algorithm can struggle to converge to the optimal solution. \n",
    "\n",
    "You might need to reduce the difficulty of the task by limiting the number of parameters that the algorithm has to learn. This can be achieved in two ways (that you can combine):\n",
    "* by reducing the dimensionality $d$, e.g. using PCA.\n",
    "* by imposing constraints on the elements of the covariance matrices. Obviously, this will affect your clustering result.\n",
    "\n",
    "<font color='red'>Task: \n",
    "* fit a Gaussian Mixture with 3 components on the sample data below and inspect the density plot. \n",
    "* Experiment with the four different values of the <tt>covariance_type</tt> hyperparameter of the <tt>GaussianMixture</tt>: <tt>full</tt>, <tt>spherical</tt>, <tt>diag</tt>, <tt>tied</tt>. \n",
    "* What is the effect of this hyperparameter on the size, orientation and shape of the Gaussians?\n",
    "* How much does each hyperparameter reduce the number of parameters to be estimated?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=35, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=5, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z,\n",
    "                 norm=LogNorm(vmin=1.0, vmax=30.0),\n",
    "                 levels=np.logspace(0, 2, 12))\n",
    "    plt.contour(xx, yy, Z,\n",
    "                norm=LogNorm(vmin=1.0, vmax=30.0),\n",
    "                levels=np.logspace(0, 2, 12),\n",
    "                linewidths=1, colors='k')\n",
    "\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    #plt.contour(xx, yy, Z,\n",
    "    #            linewidths=2, colors='r', linestyles='dashed')\n",
    "    \n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "    plot_centroids(clusterer.means_, clusterer.weights_)\n",
    "\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit your GMMS here and answer the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the number of components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With k-means, we could use the inertia or silhouette score to select the appropriate number of clusters. But with Gaussian Mixtures, it is not possible to use these metrics because they are not reliable when the clusters are not spherical or have different sizes.\n",
    "\n",
    "Instead, you can try to find the model that minimizes a *theoretical information criterion*, such as the *Bayesian information criterion* (BIC) or the *Akaike information criterion* (AIC). These values are calculated from the number of samples in your dataset, the number of parameters learned by the model and the maximized value of the likelihood. \n",
    "\n",
    "Both BIC and AIC penalize models that have more parameters to learn (e.g. more components) and reward models that fit the data well. They often end up selecting the same model. When they differ, the model selected by the BIC tends to be simpler (fewer parameters) than the one selected by AIC, but tends to not fit the data quite well as well. This is especially true for larger datasets. \n",
    "\n",
    "Both values for a trained GMM can be retrieved via the <tt>GaussianMixture.bic()</tt> and <tt>GaussianMixture.aic()</tt> methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the IRIS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply everything you have learned so far.\n",
    "\n",
    "<font color='red'>Task:\n",
    "* Fit a Gaussian Mixture with varying number of components to the original data. You do not need to apply dimensionality reduction (if you would use PCA, you would have to normalize your data first!). Set the covariance type to <tt>full</tt>. Plot the BIC and AIC as a function of the number of components. What is the best no. you can get?\n",
    "* Repeat the previous question, but now for the three other types of covariance matrices. What combination of no. of components and covariance matrix type gives you the best result (lowest BIC and AIC)?\n",
    "* Optional: compare your clustering results with the ground truth labels. You can do this with the <tt>adjusted_rand_score</tt> metric from <tt>sklearn.metrics.cluster</tt>. What percentage of the samples is in the correct cluster?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and preprocessing data\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "iris = load_iris()\n",
    "iris_data = pd.DataFrame(np.concatenate((iris.data, np.array([iris.target]).T), axis=1), columns=iris.feature_names + ['target'])\n",
    "\n",
    "#split data in features and in labels\n",
    "iris_features = iris_data.iloc[:,0:4]\n",
    "iris_labels = iris_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETER SEARCH FOR FITTING GAUSSIAN MIXTURE ON ORIGINAL 4-D DATA\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other applications of generative modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we studied maximum likelihood estimation in the context of clustering. With MLE, we try to select from a family of distributions the member that best approximates the true underlying - but unknown - data distribution $p_{data}$ that actually **generated** the data.\n",
    "<p float=\"left\">\n",
    "  <img src=\"./images/generativemodelling.png\" width=\"200\" />\n",
    "  <img src=\"./images/learning_2.png\" width=\"200\" />   \n",
    "</p>\n",
    "\n",
    "This process of **generative modelling** is useful in many other applications, of which we will study sampling and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Olivetti dataset\n",
    "Load the Olivetti dataset, which contains 400 grayscale 64 $\\times$ 64 pixel images of faces. 40 different perons were photographed (10 times each).\n",
    "Each image is described with a feature vector of dimension 64 $\\times$ 64 $=$ 4096. Each feature is the grey scale value of a particular pixel. Pixels are iterated over from left to right, row by row. Note that the grey scales values have already been normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "faces=datasets.fetch_olivetti_faces()\n",
    "print(faces.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_olivetti_samples(images):\n",
    "    total_persons=40\n",
    "    no_persons=5\n",
    "    no_imagesperperson=10\n",
    "    #Creating no_persons X no_imagesperperson subplots in  18x9 figure size\n",
    "    \n",
    "    fig, axarr=plt.subplots(nrows=no_persons, ncols=no_imagesperperson, figsize=(18, 9))\n",
    "    #For easy iteration flattened subplots matrix to array\n",
    "    axarr=axarr.flatten()\n",
    "    \n",
    "    #sample 5 user ids, and show all pictures of that person    \n",
    "    selected=random.sample(range(0,total_persons), no_persons)\n",
    "    print(selected)\n",
    "    #iterating over selected users\n",
    "    for i in range(0,no_persons):\n",
    "        for j in range(0,no_imagesperperson):\n",
    "            axarr[i*no_imagesperperson+j].imshow(images[selected[i]*no_imagesperperson+j].reshape(64,64), cmap='gray')    \n",
    "    \n",
    "show_olivetti_samples(faces.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a GMM to a distribution of greyscale images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model these images as being sampled according to an underlying density function $p_{data}(x_1,...,x_{4096})$. Each feature can take a value between 0 and 1. Intuitively, you can indeed imagine that only a very small number of all possible 4096-tuples will result in the picture of a face (most combinations would give you noise). Each image in our dataset is a \"good\" combination of pixel values. Changing randomly one pixel of an image will still result in a very similar image of a face. But if you start moving further away in feature space from the original point by randomly changing pixel values, you will ultimately end up with noise. So you can imagine that there is a region in the 4096 feature space that corresponds with pictures of this person. Similarly, there will be another region (cluster) of points corresponding to the pictures of another person. The true density distribution $p_{data}$ will have a high value in these regions of the feature space, and very low values in regions corresponding with noisy images.\n",
    "\n",
    "Our aim is to approximate this density function with a Gaussian Mixture Model $p_{model}$ that we learn from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a GMM on data of such a high dimension is infeasible.<br><br>\n",
    "\n",
    "<font color='red'>\n",
    "Task: reduce the dimensionality of your data with PCA. Select an appropriate number of components to explain 99% of the variance. (hint: have a look at possible arguments for the <tt>PCA</tt> estimator.)   \n",
    "</font>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task:\n",
    "* Fit a GMM with the selected number of components on your data with reduced dimensionality.\n",
    "* Use the BIC and AIC to determine an optimal number of clusters. You can use a <tt>full</tt> covariance matrix. Hint: avoid doing a search over a large number of components. The number is much lower than 40!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling new faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you take $N$ samples from a given probability distribution, the majority of the samples would be in feature regions having a high probability density. If we sample from our learned Gaussian Mixture Model, we should thus obtain with a high probability images that resemble faces in our dataset! \n",
    "<br><br>\n",
    "\n",
    "<font color='red'>Task: \n",
    "* Use the <tt>sample()</tt> method to sample 10 feature vectors from your fitted <tt>GaussianMixture</tt>.\n",
    "* These samples are taken from the reduced dimensionality space and do not reflect yet actual images. We can project them back to the original $64 \\times 64$ space by the <tt>inverse_transform()</tt> method of PCA. Show the generated images. Does any of your generated images resemble a person from the dataset? \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the generated images are blurry. This is due to the Gaussian distribution, who doesn't allow to model very sharp (peaked) distributions. Another reason is that some information was lost due to the dimensionality reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative models can also be used for anomaly detection. New samples, e.g. a new measurement or a new photo, that have a very low probability according to the learned GMM will likely be an outlier to the training dataset.\n",
    "\n",
    "<font color='red'>\n",
    "Run the code below to visualize some $64 \\times 64$ greyscale images that are valid but that are outliers because they do not contain faces.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "from skimage import io\n",
    "\n",
    "images=['./images/bird.jpg','./images/cat.jpg','./images/dog.jpg','./images/flower.jpg']\n",
    "\n",
    "for image in images:\n",
    "    img = io.imread(image)    \n",
    "    imgGray = color.rgb2gray(img)         \n",
    "    fig, axs = plt.subplots(1,2, figsize=(8, 8))\n",
    "    axs = axs.ravel()\n",
    "    axs[0].imshow(img)    \n",
    "    axs[1].imshow(imgGray,cmap='gray')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Task:\n",
    "* Compare the score of the above images with the score of a few samples in the training set, using the <tt>score_samples()</tt> method of <tt>GaussianMixture</tt>. This score is not a probability, and also not a probability density, but the logarithm of the probability density. Remember that you trained your GMM on the PCA-reduced data, so don't forget to transform these images!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate here the score for each image in the dataset\n",
    "for image in images:\n",
    "    img = io.imread(image)\n",
    "    imgGray = color.rgb2gray(img) \n",
    "    imgFlattenedShaped = imgGray.flatten().reshape(1,-1) #bring in correct from to apply PCA on.\n",
    "    #to do\n",
    "\n",
    "#calculate here the score of a few samples in your trainingset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude this part, let's have a look at the score of two face crops that were not in the dataset. The pictures of Matteo Simoni and Angelina Jolie have both been scaled to 64x64. The crop is similar to the dataset. The scores should be lower than the scores for items in the trainingset, but higher than the scores for an image of a flower, dog or cat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "from skimage import io\n",
    "\n",
    "images=['./images/matteo.jpg','./images/angelina.jpg']\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Gaussian Mixtures and the method of density modelling are very powerful. Oftentimes, you have to reduce the computational complexity by dimensionality reduction and by imposing constraints on the covariance matrices.\n",
    "\n",
    "Another caveat is that this problem performs not so good when the distribution of your feature values is very skewed. In this case, you should first apply an additional transformation to your features to reduce the skew. A very nice example can be found on this [blog](https://www.kaggle.com/allunia/hidden-treasures-in-our-groceries).\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "The following sources were consulted to set up this notebook:\n",
    " * http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html\n",
    " * https://mas-dse.github.io/DSE210/Additional%20Materials/gmm.pdf\n",
    " * https://www.analyticsvidhya.com/blog/2019/10/gaussian-mixture-models-clustering/\n",
    " * https://www.kaggle.com/bburns/iris-exploration-pca-k-means-and-gmm-clustering\n",
    " * https://scipython.com/blog/visualizing-the-bivariate-gaussian-distribution/\n",
    " * https://www.kaggle.com/bburns/iris-exploration-pca-k-means-and-gmm-clustering/notebook\n",
    " * https://deepgenerativemodels.github.io\n",
    " * Alpaydin E, Introduction to Machine Learning (4th edition).\n",
    " "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98adae8b0ef3e761a934d1986d78e3394b6e576c206e986053ae8bc97854a3db"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
